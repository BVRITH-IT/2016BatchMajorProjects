{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EnglishToHindi.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1YPvNwOVDGSdRdvMC_HsxKPTAlfNIYzDl","authorship_tag":"ABX9TyOOKYh9rkMHRpYLEy2Nu6r+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"00dd5srHylYU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"a23b992a-9044-4c5a-bd8a-0fc2f270381d","executionInfo":{"status":"ok","timestamp":1585029798926,"user_tz":-330,"elapsed":2873,"user":{"displayName":"V Vishwanath","photoUrl":"","userId":"03050009671869879534"}}},"source":["% tensorflow_version 2.x\n","\n","import re\n","import os\n","import time\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from string import digits\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Layer, Embedding, GRU, Dense\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy as SCC"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v-GkXGZiy6lm","colab_type":"code","colab":{}},"source":["class Translator:\n","    class _Encoder(Model):\n","        def __init__(self, size, embedding_dim, units, batch_size):\n","            super(Translator._Encoder, self).__init__()\n","            self.batch_size = batch_size\n","            self.units = units\n","            self.embedding = Embedding(size, embedding_dim)\n","            self.gru = GRU(self.units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n","\n","        def call(self, x, hidden):\n","            x = self.embedding(x)\n","            return self.gru(x, initial_state = hidden)\n","\n","        def init_hidden(self):\n","            return tf.zeros((self.batch_size, self.units))\n","\n","\n","    class _Attention(Layer):\n","        def __init__(self, units):\n","            super(Translator._Attention, self).__init__()\n","            self.X = Dense(units)\n","            self.Y = Dense(units)\n","            self.V = Dense(1)\n","\n","        def call(self, query, values):\n","            score = self.V(tf.nn.tanh(self.X(tf.expand_dims(query, 1)) + self.Y(values)))\n","            attention_weights = tf.nn.softmax(score, axis=1)\n","            context_vector = attention_weights * values\n","            context_vector = tf.reduce_sum(context_vector, axis=1)\n","            return context_vector, attention_weights\n","\n","\n","    class _Decoder(Model):\n","        def __init__(self, size, embedding_dim, units, batch_size):\n","            super(Translator._Decoder, self).__init__()\n","            self.batch_size = batch_size\n","            self.units = units\n","            self.embedding = tf.keras.layers.Embedding(size, embedding_dim)\n","            self.gru = tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n","            self.fc = tf.keras.layers.Dense(size)\n","            self.attention = Translator._Attention(self.units)\n","\n","        def call(self, x, hidden, enc_output):\n","            context_vector, attention_weights = self.attention(hidden, enc_output)\n","            x = self.embedding(x)\n","            x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","            output, state = self.gru(x)\n","            output = tf.reshape(output, (-1, output.shape[2]))\n","            return self.fc(output), state, attention_weights\n","\n","\n","    def __init__(self, inp_lang_samples, out_lang_samples):\n","        inp_lang_samples = [self._preprocess(i) for i in inp_lang_samples]\n","        out_lang_samples = [self._preprocess(i) for i in out_lang_samples]\n","\n","        inp_tensors, self._inp_tokenizer = self._tokenize(inp_lang_samples)\n","        out_tensors, self._out_tokenizer = self._tokenize(out_lang_samples)\n","        self.max_inp_len = max(len(tensor) for tensor in inp_tensors)\n","        self.max_out_len = max(len(tensor) for tensor in out_tensors)\n","\n","        self.train_inp, _, self.train_out, _ = train_test_split(inp_tensors, out_tensors, test_size=0.2)\n","        self.BUFFER_SIZE = len(self.train_inp)\n","        self.BATCH_SIZE = 64\n","        self.steps_per_epoch = self.BUFFER_SIZE // self.BATCH_SIZE\n","        self.embedding_dim = 256\n","        self.units = 1024\n","\n","\n","    def _preprocess(self, sentence):\n","        sentence = sentence.lower().strip()\n","        sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n","        sentence = re.sub(r'[\" \"]+', \" \", sentence)\n","        remove_digits = str.maketrans('', '', digits)\n","        sentence = sentence.translate(remove_digits)\n","        sentence = re.sub('[२३०८१५७९४६|]', \"\", sentence)\n","        sentence = sentence.rstrip().strip()\n","        sentence = '<s> ' + sentence + ' <e>'\n","        return sentence\n","\n","\n","    def _tokenize(self, text):\n","        tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n","        tokenizer.fit_on_texts(text)\n","        text_tensors = tokenizer.texts_to_sequences(text)\n","        text_tensors = tf.keras.preprocessing.sequence.pad_sequences(text_tensors, padding='post')\n","        return text_tensors, tokenizer\n","\n","\n","    def train(self):\n","        inp_size = len(self._inp_tokenizer.word_index) + 1\n","        out_size = len(self._out_tokenizer.word_index) + 1\n","        self._encoder = self._Encoder(inp_size, self.embedding_dim, self.units, self.BATCH_SIZE)\n","        self._decoder = self._Decoder(out_size, self.embedding_dim, self.units, self.BATCH_SIZE)\n","\n","        self._optimizer = Adam()\n","        self._loss_object = SCC(from_logits=True, reduction='none')\n","\n","        self._checkpoint = tf.train.Checkpoint(optimizer=self._optimizer, encoder=self._encoder, decoder=self._decoder)\n","\n","        if not os.path.exists('checkpoints'):\n","            dataset = tf.data.Dataset.from_tensor_slices((self.train_inp, self.train_out)).shuffle(self.BUFFER_SIZE)\n","            dataset = dataset.batch(self.BATCH_SIZE, drop_remainder=True)\n","            n_epochs = 12\n","\n","            for i in range(n_epochs):\n","                start = time.time()\n","                print(f'EPOCH {i+1}')\n","\n","                hidden = self._encoder.init_hidden()\n","                total_loss = 0\n","\n","                for (batch, (inp, out)) in enumerate(dataset.take(self.steps_per_epoch)):\n","                    batch_loss = self._training_step(inp, out, hidden)\n","                    total_loss += batch_loss\n","\n","                    if batch % 50 == 0: print(f'Completed {batch} batches, Loss : {batch_loss.numpy():.4f}')\n","                        \n","                if (i+1) % 2 == 0: self._checkpoint.save(file_prefix = os.path.join('./checkpoints', 'ckpt'))\n","\n","                print(f'Epoch {i+1} completed, Loss : {(total_loss / self.steps_per_epoch):.4f}')\n","                print(f'Time taken : {time.time() - start:.4f} s')\n","                print('--'*20 + '\\n')\n","\n","        self._checkpoint.restore(tf.train.latest_checkpoint('./checkpoints'))  \n","\n","\n","    def _loss_function(self, real, prediction):\n","        mask = tf.math.logical_not(tf.math.equal(real, 0))\n","        loss = self._loss_object(real, prediction)\n","        mask = tf.cast(mask, dtype=loss.dtype)\n","        loss *= mask \n","        return tf.reduce_mean(loss)\n","\n","\n","    @tf.function\n","    def _training_step(self, inp, out, hidden):\n","        loss = 0\n","\n","        with tf.GradientTape() as tape:\n","            output, dec_hidden = self._encoder(inp, hidden)\n","            dec_input = tf.expand_dims([self._out_tokenizer.word_index['<s>']] * self.BATCH_SIZE, 1)\n","\n","            for t in range(1, out.shape[1]):\n","                predictions, dec_hidden, _ = self._decoder(dec_input, dec_hidden, output)\n","                loss += self._loss_function(out[:, t], predictions)\n","                dec_input = tf.expand_dims(out[:, t], 1)\n","                \n","        batch_loss = (loss / int(out.shape[1]))\n","        variables = self._encoder.trainable_variables + self._decoder.trainable_variables\n","        gradients = tape.gradient(loss, variables)\n","        self._optimizer.apply_gradients(zip(gradients, variables))\n","\n","        return batch_loss\n","\n","    def translate(self, sentence):\n","        sentence = self._preprocess(sentence)\n","\n","        inputs = [self._inp_tokenizer.word_index[i] for i in sentence.split()]\n","        inputs = pad_sequences([inputs], maxlen=self.max_inp_len, padding='post')\n","        inputs = tf.convert_to_tensor(inputs)\n","\n","        result = ''\n","\n","        hidden = [tf.zeros((1, self.units))]\n","        output, dec_hidden = self._encoder(inputs, hidden)\n","        dec_input = tf.expand_dims([self._out_tokenizer.word_index['<s>']], 0)\n","\n","        for _ in range(self.max_out_len):\n","            pred, dec_hidden, _ = self._decoder(dec_input, dec_hidden, output)\n","            index = tf.argmax(pred[0]).numpy()\n","            result += self._out_tokenizer.index_word[index] + ' '\n","            if self._out_tokenizer.index_word[index] == '<e>': return result[:-4].strip()\n","            dec_input = tf.expand_dims([index], 0)\n","            \n","        return result[:-4].strip()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VWBoIOOozZVG","colab_type":"code","colab":{}},"source":["data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/EnglishToHindi/data.csv')\n","\n","data = data[data['source'] == 'ted']\n","data = data[~pd.isnull(data['english_sentence'])]\n","data.drop_duplicates(inplace=True)\n","\n","data = data.sample(n=35000, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rEY3ZtX3zcjV","colab_type":"code","colab":{}},"source":["inp_lang = [i for i in data['english_sentence']]\n","out_lang = [i for i in data['hindi_sentence']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yJ9r2LsMzuzT","colab_type":"code","colab":{}},"source":["translator = Translator(inp_lang, out_lang)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dOe-pvUizwkY","colab_type":"code","colab":{}},"source":["translator.train()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dkRA9f9hzybi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"74c8ebd1-496f-4a9e-d90d-d43c44b4eb27","executionInfo":{"status":"ok","timestamp":1585029816729,"user_tz":-330,"elapsed":3346,"user":{"displayName":"V Vishwanath","photoUrl":"","userId":"03050009671869879534"}}},"source":["translator.translate('What are you doing?')"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'आप क्या कर रहे हो ?'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"3lLRQlhHHgbB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"dfe5bb82-aaa2-443b-c3bc-c8faeb19504a","executionInfo":{"status":"ok","timestamp":1585029819523,"user_tz":-330,"elapsed":781,"user":{"displayName":"V Vishwanath","photoUrl":"","userId":"03050009671869879534"}}},"source":["translator.translate('What is your name?')"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'आपका नाम क्या है ?'"]},"metadata":{"tags":[]},"execution_count":8}]}]}